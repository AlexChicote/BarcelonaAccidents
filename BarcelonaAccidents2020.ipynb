{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from functions_barcelona import posant_accents,utmToLatLng,\\\n",
    "                    ped_to_angles,setmana_a_angles,mes_a_angles,cause_to_angles\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from datetime import date, datetime, timedelta\n",
    "from functions_barcelona import getting_daily_weather,getting_next_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://opendata-ajuntament.barcelona.cat/data/api/3/action/package_search?rows=1000')\n",
    "response= r.json()\n",
    "\n",
    "\n",
    "def concatenating_dataframes(filter1):\n",
    "    \n",
    "    files=response['result']['results']\n",
    "    for num, file in enumerate(files):\n",
    "        \n",
    "        if ('accidents-' +filter1+'gu') in file['name']:\n",
    "            print(file['name'])\n",
    "            filter_list=[]\n",
    "            \n",
    "            for fitxer in file['resources']:\n",
    "\n",
    "                if fitxer['format']=='CSV':\n",
    "\n",
    "                    print(fitxer['name'],fitxer['format'])\n",
    "                    #print(fitxer['url'])\n",
    "                    try:\n",
    "                        filter_list.append(pd.read_csv(fitxer['url']))\n",
    "                    except:\n",
    "                        try:\n",
    "                            filter_list.append(pd.read_csv(fitxer['url'],encoding='ISO-8859-15'))\n",
    "                        except:\n",
    "\n",
    "                            filter_list.append(pd.read_csv(fitxer['url'],sep=';',encoding='ISO-8859-1'))\n",
    "\n",
    "            column_names=filter_list[0].columns\n",
    "            data=pd.DataFrame()\n",
    "            for df in filter_list:\n",
    "                df=pd.DataFrame.from_records(df.values)\n",
    "                data=pd.concat([data,df])\n",
    "                #print(data.shape)\n",
    "            data.columns=column_names\n",
    "            data=data.reset_index()\n",
    "    try:\n",
    "        return data\n",
    "    except:\n",
    "        print('NO FILE WITH THAT FILTER')\n",
    "    print('CONCATENATING DONE')\n",
    "#causes_df= concatenating_dataframes('causes-')\n",
    "\n",
    "noms=['causes-', 'persones-','tipus-', 'vehicles-','']\n",
    "dict_noms={}\n",
    "for nom in noms:\n",
    "    dict_noms[nom[:-1]]=concatenating_dataframes(nom)\n",
    "\n",
    "pickle.dump( dict_noms, open( \"./data/dataframes_dict.pkl\", \"wb\" ) )\n",
    "#favorite_color = pickle.load( open( \"dataframes_dict.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning ACCIDENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_noms = pickle.load( open( \"./data/dataframes_dict.pkl\", \"rb\" ) )\n",
    "acc= dict_noms[''].copy()\n",
    "\n",
    "#acc.columns\n",
    "index_to_drop=acc[acc.isnull().sum(axis=1)>10].index\n",
    "acc=acc.drop(index_to_drop)\n",
    "acc['Nom_carrer']=acc['Nom_carrer'].fillna('Casetes')\n",
    "for column in acc:\n",
    "    if 'Coor' in column:\n",
    "        print(column)\n",
    "        acc[column]=[str(coor).replace(',','.') for coor in acc[column]]\n",
    "        acc[column]= acc[column].astype(float)\n",
    "for column in acc:\n",
    "    acc[column]=acc[column].apply(posant_accents)\n",
    "old_column_names=acc.columns\n",
    "acc=acc.drop(['Num_postal_caption','Longitud','Latitud','index','Codi_barri', 'Dia_setmana','Mes_any',\n",
    "             'Descripcio_torn', 'Descripcio_tipus_dia'],axis=1)\n",
    "print('Number of nulls:',acc.isnull().sum().sum())\n",
    "\n",
    "acc['Codi_districte']=[x if x!='Desconegut' else -1 for x in acc['Codi_districte']]\n",
    "acc['Codi_carrer']=acc['Codi_carrer'].astype(int)\n",
    "#acc['Codi_barri']=[str(x)[-2:].replace('-','') for x in acc.Codi_barri]\n",
    "tuple_features=[('Nom_carrer','Codi_carrer'),('Nom_districte','Codi_districte')]\n",
    "for tup in tuple_features:\n",
    "    if acc[tup[0]].nunique()==acc[tup[1]].nunique():\n",
    "        acc=acc.drop(tup[1],axis=1)\n",
    "        print(f\"I dropped {tup[1]}\")\n",
    "\n",
    "print(acc.shape)\n",
    "\n",
    "acc.columns=['num_incident', 'district', 'neighborhood', 'street_code',\n",
    "       'street_name', 'weekday', 'year', 'month', 'day', 'hour', 'ped_cause',\n",
    "       'num_deaths', 'num_minorly_injured', 'num_severly_injured',\n",
    "       'num_victims', 'num_vehicles', 'utm_y',\n",
    "       'utm_x']\n",
    "##Translating to English\n",
    "\n",
    "acc['ped_cause']=acc['ped_cause'].apply(ped_to_angles)\n",
    "acc['weekday']=acc['weekday'].apply(setmana_a_angles)\n",
    "acc['month']=acc['month'].apply(mes_a_angles)\n",
    "acc['num_incident']=[x.strip() for x in acc['num_incident']]\n",
    "for col in ['year','day','hour','num_deaths','num_minorly_injured','num_severly_injured','num_victims','num_vehicles']:\n",
    "    acc[col]=acc[col].astype(int)\n",
    "##eliminatong duplictaes that have 2 ped cause: I am losing the second one.\n",
    "index_to_drop= acc[acc['num_incident'].duplicated()].index\n",
    "acc=acc.drop(index_to_drop).reset_index()\n",
    "acc.to_csv('./data/accidents_only2020.csv')\n",
    "print('Accidents: ', acc.shape)\n",
    "\n",
    "##adding causes\n",
    "causes= dict_noms['causes'].reset_index().copy()\n",
    "\n",
    "\n",
    "columns_to_keep=['Descripcio_causa_mediata', 'Numero_expedient']\n",
    "\n",
    "causes=causes[columns_to_keep]\n",
    "causes.columns=['cause','num_incident']\n",
    "causes['num_incident']=[x.strip() for x in causes['num_incident']]\n",
    "causes['cause']=causes.cause.apply(posant_accents).apply(cause_to_angles)\n",
    "print(causes.shape)\n",
    "causes=causes.drop_duplicates('num_incident')\n",
    "\n",
    "\n",
    "causes['cause']=causes['cause'].fillna('Unknown')\n",
    "causes.to_csv('./data/causes2020.csv')\n",
    "total= pd.merge(acc,causes, how='left',on='num_incident')\n",
    "print('causes: ', causes.shape, 'Total: ',total.shape)\n",
    "\n",
    "\n",
    "\n",
    "##people\n",
    "\n",
    "people= dict_noms['persones'].reset_index().copy()\n",
    "\n",
    "columns_to_add=['Numero_Expedient','Desc_Tipus_vehicle_implicat', 'Descripcio_sexe', 'Edat',\n",
    "                'Descripcio_tipus_persona', 'Descripcio_Lloc_atropellament_vianat',\n",
    "                'Descripcio_Motiu_desplaçament_vianant',\n",
    "                'Descripcio_Motiu_desplaçament_conductor', 'Descripcio_victimitzacio',]\n",
    "people.columns=[posant_accents(col) for col in people.columns]\n",
    "anual_dict={}\n",
    "for any_ in sorted(people['NK_ Any'].unique()):\n",
    "    if any_ in [2010,2011,2012,2013]:\n",
    "        df=people[people['NK_ Any']==any_].copy()\n",
    "        df.rename(columns={'Edat':'Descripcio_tipus_persona_','Descripcio_tipus_persona':'Edat_',\\\n",
    "        'Descripcio_Motiu_desplaçament_conductor':'Coordenada_UTM_Y_',\n",
    "        'Descripcio_Motiu_desplaçament_vianant': 'Coordenada_UTM_X_',\n",
    "        'Descripcio_Lloc_atropellament_vianat':'Descripcio_victimitzacio_' },inplace=True)\n",
    "        df=df.drop(df.columns[-5:],axis=1)\n",
    "        #columnes=[]\n",
    "        df.columns=[col if col[-1]!='_' else col[:-1] for col in df.columns]\n",
    "        columnes=[col for col in df.columns if col in columns_to_add]\n",
    "        df=df[columnes].copy()\n",
    "        anual_dict[any_]=df\n",
    "    elif any_ in [2014,2015]:\n",
    "        df=people[people['NK_ Any']==any_].copy()\n",
    "        df.rename(columns={'Descripcio_Motiu_desplaçament_vianant':'Coordenada_UTM_Y_','Descripcio_Lloc_atropellament_vianat':'Coordenada_UTM_X_',\\\n",
    "        'Descripcio_tipus_persona':'Descripcio_victimitzacio_',\n",
    "        'Descripcio_sexe': 'Descripcio_tipus_persona_',\n",
    "        'Desc_Tipus_vehicle_implicat':'Descripcio_sexe_',\n",
    "         'Descripcio_causa_vianant':  'Desc_Tipus_vehicle_implicat_'   },inplace=True)\n",
    "        df=df.drop(df.columns[-6:],axis=1)\n",
    "        df.columns=[col if col[-1]!='_' else col[:-1] for col in df.columns]\n",
    "        columnes=[col for col in df.columns if col in columns_to_add]\n",
    "        df=df[columnes].copy()\n",
    "        anual_dict[any_]=df\n",
    "    elif any_ in [2016,2017,2018]:\n",
    "        df=people[people['NK_ Any']==any_].copy()\n",
    "        df.rename(columns={'Descripcio_Motiu_desplaçament_vianant':'Descripcio_victimitzacio_','Descripcio_Lloc_atropellament_vianat':'Descripcio_situacio',\n",
    "        },inplace=True)\n",
    "        df=df.drop(df.columns[-6:],axis=1)\n",
    "        df.columns=[col if col[-1]!='_' else col[:-1] for col in df.columns]\n",
    "        columnes=[col for col in df.columns if col in columns_to_add]\n",
    "        df=df[columnes].copy()\n",
    "        anual_dict[any_]=df\n",
    "    elif any_ in [2019,2020]:\n",
    "        df=people[people['NK_ Any']==any_].copy()\n",
    "        columnes=[col for col in df.columns if col in columns_to_add]\n",
    "        df=df[columnes].copy()\n",
    "        anual_dict[any_]=df\n",
    "\n",
    "mapping_columns={'Numero_Expedient':'num_incident',\n",
    "             'Desc_Tipus_vehicle_implicat':'vehicle',\n",
    "             'Descripcio_sexe':'gender',\n",
    "             'Edat':'age',\n",
    "            'Descripcio_tipus_persona':'people_role',\n",
    "             'Descripcio_Lloc_atropellament_vianat':'run_over_location',\n",
    "             'Descripcio_victimitzacio': 'level_injuries',\n",
    "              'Descripcio_Motiu_desplaçament_vianant':'peds_activity',\n",
    "            'Descripcio_Motiu_desplaçament_conductor':'drivers_activity'}\n",
    "people=pd.DataFrame()\n",
    "for key in anual_dict.keys():\n",
    "    #print(key, anual_dict[key].shape)\n",
    "    people= pd.concat([people,anual_dict[key].rename(columns=mapping_columns)])\n",
    "    #print(people.shape)\n",
    "    ##too many nulls.\n",
    "for col in people:\n",
    "    if people[col].isnull().sum()>10000:\n",
    "        people=people.drop(col,axis=1)\n",
    "people=people.drop('level_injuries',axis=1)\n",
    "people['num_incident']=[x.strip() for x in people['num_incident']]\n",
    "people['age']=people.age.astype(str)\n",
    "compressed_people= people.groupby('num_incident').agg(lambda x : ','.join(x)).reset_index()\n",
    "people.to_csv('./data/people2020.csv')\n",
    "total= pd.merge(total,compressed_people, how='left',on='num_incident')\n",
    "\n",
    "total.sample(5)\n",
    "print('people: ', people.shape, 'Total: ',total.shape)\n",
    "\n",
    "##TYPE\n",
    "tipus=dict_noms['tipus'].copy()\n",
    "type_accident_map={'Atropellament': 'run_over',\n",
    "         'Col.lisio lateral': 'lateral_collision',\n",
    "        'Xoc contra element estatic': 'crash_into_stationary',\n",
    "      'Abast': 'rear-end_collision',\n",
    "       'Col.lisio frontal':'frontal_collision',\n",
    "      'Col.lisio fronto-lateral':'frontal-lateral_collision',\n",
    "      'Caiguda (dues rodes)':'fall--motorcycle',\n",
    "      'Abast multiple':'multiple_rear-end_collision',\n",
    "      'Caiguda interior vehicle':'fall_inside_vehicle',\n",
    "      'Altres':'Other_types',\n",
    "      'Bolcada (mes de dues rodes)':'overturning',\n",
    "      'Desconegut':'unknown',\n",
    "      'Sortida de via amb xoc o col.lisio':'run-off_with_crash_or_collision',\n",
    "      'Encalç':'rear-end_collision',\n",
    "      'Sortida de via amb bolcada':'run-off_with_overturning',\n",
    "      'Xoc amb animal a la calçada':'crash_into_animal_on_road',\n",
    "      'Resta sortides de via':'run-off_not_included_previously'}\n",
    "\n",
    "tipus['Descripcio_tipus_accident']=tipus['Descripcio_tipus_accident'].apply(posant_accents).map(type_accident_map)\n",
    "tipus=tipus[['Numero_expedient','Descripcio_tipus_accident',]].copy()\n",
    "tipus.columns=['num_incident','accident_type']\n",
    "tipus['num_incident']=[x.strip() for x in tipus.num_incident]\n",
    "tipus=tipus.dropna()\n",
    "tipus=tipus.groupby('num_incident')['accident_type'].agg(lambda x: ','.join(x)).reset_index()\n",
    "total= pd.merge(total,tipus, how='left',on='num_incident')\n",
    "tipus.to_csv('./data/types2020.csv')\n",
    "\n",
    "print('type: ', tipus.shape, 'Total: ',total.shape)\n",
    "\n",
    "##VEHICLE***Model no surt a gtots els anys\n",
    "\n",
    "vehicles= dict_noms['vehicles'].copy()\n",
    "columns_to_add=['Numero_expedient', 'Descripcio_model', 'Descripcio_marca',\n",
    "       'Descripcio_color', 'Descripcio_carnet', 'Antiguitat_carnet' ]\n",
    "\n",
    "anual_dict={}\n",
    "\n",
    "for any_ in sorted(vehicles['NK_Any'].unique()):\n",
    "    if any_ in [2010,2011,2013,2014,2015,2016,2017]:\n",
    "        df=vehicles[vehicles['NK_Any']==any_].copy()\n",
    "        df.rename(columns={'Descripcio_tipus_vehicle':'Descripcio_model_','Descripcio_model':'Descripcio_marca_',\\\n",
    "        'Descripcio_marca':'Descripcio_color_',\n",
    "        'Descripcio_color': 'Descripcio_carnet_',\n",
    "        'Descripcio_carnet':'Antiguitat_carnet_' },inplace=True)\n",
    "        df=df.drop(df.columns[-5:],axis=1)\n",
    "        df.columns=[col if col[-1]!='_' else col[:-1] for col in df.columns]\n",
    "        columnes=[col for col in df.columns if col in columns_to_add]\n",
    "        df=df[columnes].copy()\n",
    "        anual_dict[any_]=df\n",
    "    elif any_ in [2018,2019,2020]:\n",
    "        df=vehicles[vehicles['NK_Any']==any_].copy()        \n",
    "        columnes=[col for col in df.columns if col in columns_to_add]\n",
    "        df=df[columnes].copy()\n",
    "        anual_dict[any_]=df\n",
    "\n",
    "vehicles=pd.DataFrame()\n",
    "for key in anual_dict.keys():\n",
    "    vehicles=pd.concat([vehicles,anual_dict[key]],)\n",
    "    \n",
    "\n",
    "vehicles.columns=['num_incident', 'vehicle_model', 'vehicle_brand',\n",
    "       'vehicle_color', 'type_license', 'signority_license']\n",
    "\n",
    "vehicles=vehicles.dropna()\n",
    "vehicles['num_incident']=[x.strip() for x in vehicles.num_incident]\n",
    "vehicles=vehicles.groupby('num_incident').agg(lambda x: ','.join(x)).reset_index()\n",
    "vehicles.to_csv('./data/vehicles2020.csv')\n",
    "total= pd.merge(total,vehicles, how='left',on='num_incident')\n",
    "total.to_csv('./data/accidents2020.csv')\n",
    "print('vehicles: ', vehicles.shape, 'Total: ',total.shape)\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##REPEAT THE PROCESS FOR EACH YEAR----I have to limit the number of calls gtherefore I have to do it by year\n",
    "\n",
    "import datetime\n",
    "start_date = datetime.date(2020, 1, 1)\n",
    "end_date = datetime.date(2020, 12, 31)\n",
    "delta = datetime.timedelta(days=1)\n",
    "df_total = []\n",
    "while start_date <= end_date:\n",
    "    print(start_date)\n",
    "    df= getting_daily_weather(str(start_date))\n",
    "    df_total.append(df)\n",
    "    start_date += delta\n",
    "pd.concat(df_total).to_csv('./data/weather2020.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
